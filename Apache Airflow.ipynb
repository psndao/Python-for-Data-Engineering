{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563eca46",
   "metadata": {},
   "source": [
    "                                Apache Airflow: Workflow Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f072b4a2",
   "metadata": {},
   "source": [
    "La définition DAG (Directed Acyclic Graph) d’Apache Airflow est un moyen puissant de structurer les pipelines ETL. \n",
    "\n",
    "Principaux avantages :\n",
    "\n",
    "-  Visualisation des workflows : les DAG offrent une représentation visuelle claire de votre processus ETL\n",
    "-  Gestion des dépendances : définissez facilement les dépendances des tâches et leur ordre d’exécution\n",
    "-  Ordonnancement : fonctionnalités de planification intégrées pour les workflows récurrents\n",
    "-  Paramétrage : exécution dynamique basée sur des dates, des variables et des configurations\n",
    "-  Surveillance : suivez l’état des tâches, leur durée d’exécution et les échecs\n",
    "-  Relances et gestion des erreurs : relancez automatiquement les tâches ayant échoué et gérez les erreurs correctement\n",
    "\n",
    "En ingénierie des données, les DAG sont essentiels pour :\n",
    "\n",
    "-  Organisation des processus : décomposez les workflows ETL complexes en tâches gérables\n",
    "-  Gestion des dépendances : assurez-vous que les tâches s’exécutent dans le bon ordre\n",
    "-  Allocation des ressources : contrôlez le parallélisme et l’utilisation des ressources\n",
    "-  Surveillance et alertes : suivez l’état du pipeline et recevez des notifications en cas d’échec\n",
    "-  Documentation : workflows auto-documentés avec des relations claires entre les tâches\n",
    "\n",
    "L’exemple illustre un pipeline ETL complet avec plusieurs bonnes pratiques :\n",
    "\n",
    "-  Modularité des tâches : chaque étape du processus ETL est une tâche distincte\n",
    "-  Transfert de données : utilisez XComs pour Transférer des données entre les tâches\n",
    "-  Gestion des erreurs : Implémentation de nouvelles tentatives et de délais d’expiration\n",
    "-  Contrôles de la qualité des données : Validation des données chargées\n",
    "-  Notifications : Alerte en cas d’achèvement ou d’échec du pipeline\n",
    "-  Documentation : Inclure une documentation détaillée dans la définition du DAG\n",
    "\n",
    "\n",
    "Ce modèle est courant en ingénierie des données, où les pipelines de données complexes doivent être fiables, maintenables et observables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc1ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nom du fichier : example_etl_dag.py\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.providers.postgres.hooks.postgres import PostgresHook\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from airflow.models import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Arguments par défaut pour toutes les tâches du DAG\n",
    "default_args = {\n",
    "    'owner': 'data_engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['data_alerts@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(hours=1),\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'catchup': False,\n",
    "}\n",
    "\n",
    "# Définition du DAG\n",
    "dag = DAG(\n",
    "    'customer_order_etl_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline ETL pour les données clients et commandes',\n",
    "    schedule_interval='0 2 * * *',  # S'exécute tous les jours à 2h du matin\n",
    "    max_active_runs=1,\n",
    "    tags=['etl', 'customer', 'orders'],\n",
    "    doc_md=\"\"\"\n",
    "    # Pipeline ETL Client-Commande\n",
    "    \n",
    "    Ce DAG effectue les opérations suivantes :\n",
    "    1. Extraction des données clients et commandes\n",
    "    2. Transformation : nettoyage, enrichissement, agrégation\n",
    "    3. Chargement dans l'entrepôt de données\n",
    "    4. Contrôles qualité\n",
    "    5. Notification de succès\n",
    "    \n",
    "    ## Dépendances\n",
    "    - Nécessite que la base opérationnelle soit disponible\n",
    "    - Dépend de la complétion du DAG `daily_data_sync`\n",
    "    \n",
    "    ## Équipe\n",
    "    Équipe Data Engineering\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Fonction pour extraire les données clients\n",
    "def extract_customer_data(**context):\n",
    "    logging.info(\"Extraction des données clients\")\n",
    "    execution_date = context['execution_date']\n",
    "    pg_hook = PostgresHook(postgres_conn_id='postgres_oltp')\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT customer_id, name, email, signup_date, last_login_date, customer_segment\n",
    "    FROM customers\n",
    "    WHERE updated_at >= %s\n",
    "    \"\"\"\n",
    "    cutoff_date = execution_date - timedelta(days=1)\n",
    "    df = pg_hook.get_pandas_df(sql, parameters=[cutoff_date])\n",
    "    logging.info(f\"{len(df)} lignes client extraites\")\n",
    "    temp_file_path = f\"/tmp/customer_extract_{execution_date.strftime('%Y%m%d')}.csv\"\n",
    "    df.to_csv(temp_file_path, index=False)\n",
    "    context['ti'].xcom_push(key='customer_extract_path', value=temp_file_path)\n",
    "    return temp_file_path\n",
    "\n",
    "# Fonction pour extraire les données commandes\n",
    "def extract_order_data(**context):\n",
    "    logging.info(\"Extraction des données commandes\")\n",
    "    execution_date = context['execution_date']\n",
    "    pg_hook = PostgresHook(postgres_conn_id='postgres_oltp')\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT o.order_id, o.customer_id, o.order_date, o.total_amount, o.status,\n",
    "           oi.product_id, oi.quantity, oi.unit_price\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    WHERE o.order_date >= %s\n",
    "    \"\"\"\n",
    "    cutoff_date = execution_date - timedelta(days=1)\n",
    "    df = pg_hook.get_pandas_df(sql, parameters=[cutoff_date])\n",
    "    logging.info(f\"{len(df)} lignes commande extraites\")\n",
    "    temp_file_path = f\"/tmp/order_extract_{execution_date.strftime('%Y%m%d')}.csv\"\n",
    "    df.to_csv(temp_file_path, index=False)\n",
    "    context['ti'].xcom_push(key='order_extract_path', value=temp_file_path)\n",
    "    return temp_file_path\n",
    "\n",
    "# Fonction de transformation des données\n",
    "def transform_data(**context):\n",
    "    logging.info(\"Transformation des données\")\n",
    "    ti = context['ti']\n",
    "    customer_file = ti.xcom_pull(task_ids='extract_customer_data', key='customer_extract_path')\n",
    "    order_file = ti.xcom_pull(task_ids='extract_order_data', key='order_extract_path')\n",
    "\n",
    "    customers_df = pd.read_csv(customer_file)\n",
    "    orders_df = pd.read_csv(order_file)\n",
    "\n",
    "    # Nettoyage : suppression des doublons, valeurs manquantes\n",
    "    customers_df = customers_df.drop_duplicates(subset=['customer_id'])\n",
    "    orders_df = orders_df.drop_duplicates(subset=['order_id', 'product_id'])\n",
    "    customers_df['customer_segment'] = customers_df['customer_segment'].fillna('Unknown')\n",
    "\n",
    "    # Conversions de dates\n",
    "    customers_df['signup_date'] = pd.to_datetime(customers_df['signup_date'])\n",
    "    customers_df['last_login_date'] = pd.to_datetime(customers_df['last_login_date'])\n",
    "    orders_df['order_date'] = pd.to_datetime(orders_df['order_date'])\n",
    "\n",
    "    # Jours depuis dernière connexion\n",
    "    customers_df['days_since_last_login'] = (datetime.now() - customers_df['last_login_date']).dt.days\n",
    "\n",
    "    # Valeur vie client (LTV)\n",
    "    customer_ltv = orders_df.groupby('customer_id')['total_amount'].sum().reset_index()\n",
    "    customer_ltv.columns = ['customer_id', 'lifetime_value']\n",
    "    enriched_customers = customers_df.merge(customer_ltv, on='customer_id', how='left')\n",
    "    enriched_customers['lifetime_value'] = enriched_customers['lifetime_value'].fillna(0)\n",
    "\n",
    "    # Résumé des commandes\n",
    "    order_summary = orders_df.groupby('order_id').agg({\n",
    "        'customer_id': 'first',\n",
    "        'order_date': 'first',\n",
    "        'total_amount': 'first',\n",
    "        'status': 'first',\n",
    "        'quantity': 'sum',\n",
    "        'unit_price': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Sauvegarde des fichiers transformés\n",
    "    execution_date = context['execution_date'].strftime('%Y%m%d')\n",
    "    customer_output = f\"/tmp/transformed_customers_{execution_date}.csv\"\n",
    "    order_output = f\"/tmp/transformed_orders_{execution_date}.csv\"\n",
    "    enriched_customers.to_csv(customer_output, index=False)\n",
    "    order_summary.to_csv(order_output, index=False)\n",
    "    ti.xcom_push(key='transformed_customers_path', value=customer_output)\n",
    "    ti.xcom_push(key='transformed_orders_path', value=order_output)\n",
    "    logging.info(f\"Transformation terminée : {len(enriched_customers)} clients, {len(order_summary)} commandes\")\n",
    "    return {'customers_path': customer_output, 'orders_path': order_output}\n",
    "\n",
    "# Fonction de chargement des clients\n",
    "def load_customer_data(**context):\n",
    "    logging.info(\"Chargement des clients dans le DWH\")\n",
    "    ti = context['ti']\n",
    "    file_path = ti.xcom_pull(task_ids='transform_data', key='transformed_customers_path')\n",
    "    df = pd.read_csv(file_path)\n",
    "    pg_hook = PostgresHook(postgres_conn_id='postgres_dwh')\n",
    "\n",
    "    temp_table = 'temp_customers'\n",
    "    target_table = 'dim_customers'\n",
    "\n",
    "    pg_hook.run(f\"DROP TABLE IF EXISTS {temp_table}\")\n",
    "    pg_hook.run(f\"\"\"\n",
    "    CREATE TABLE {temp_table} (\n",
    "        customer_id INTEGER PRIMARY KEY,\n",
    "        name VARCHAR(100),\n",
    "        email VARCHAR(100),\n",
    "        signup_date DATE,\n",
    "        last_login_date TIMESTAMP,\n",
    "        customer_segment VARCHAR(50),\n",
    "        days_since_last_login INTEGER,\n",
    "        lifetime_value NUMERIC(10,2)\n",
    "    )\n",
    "    \"\"\")\n",
    "    pg_hook.insert_rows(temp_table, df.values.tolist(), df.columns.tolist(), commit_every=1000)\n",
    "    pg_hook.run(f\"\"\"\n",
    "    INSERT INTO {target_table}\n",
    "    SELECT * FROM {temp_table}\n",
    "    ON CONFLICT (customer_id)\n",
    "    DO UPDATE SET\n",
    "        name = EXCLUDED.name,\n",
    "        email = EXCLUDED.email,\n",
    "        signup_date = EXCLUDED.signup_date,\n",
    "        last_login_date = EXCLUDED.last_login_date,\n",
    "        customer_segment = EXCLUDED.customer_segment,\n",
    "        days_since_last_login = EXCLUDED.days_since_last_login,\n",
    "        lifetime_value = EXCLUDED.lifetime_value\n",
    "    \"\"\")\n",
    "    pg_hook.run(f\"DROP TABLE IF EXISTS {temp_table}\")\n",
    "    row_count = pg_hook.get_records(f\"SELECT COUNT(*) FROM {target_table}\")[0][0]\n",
    "    logging.info(f\"Chargement clients terminé, {row_count} lignes dans {target_table}\")\n",
    "    return row_count\n",
    "\n",
    "# Fonction de chargement des commandes\n",
    "def load_order_data(**context):\n",
    "    logging.info(\"Chargement des commandes dans le DWH\")\n",
    "    ti = context['ti']\n",
    "    file_path = ti.xcom_pull(task_ids='transform_data', key='transformed_orders_path')\n",
    "    df = pd.read_csv(file_path)\n",
    "    pg_hook = PostgresHook(postgres_conn_id='postgres_dwh')\n",
    "\n",
    "    temp_table = 'temp_orders'\n",
    "    target_table = 'fact_orders'\n",
    "\n",
    "    pg_hook.run(f\"DROP TABLE IF EXISTS {temp_table}\")\n",
    "    pg_hook.run(f\"\"\"\n",
    "    CREATE TABLE {temp_table} (\n",
    "        order_id INTEGER PRIMARY KEY,\n",
    "        customer_id INTEGER,\n",
    "        order_date DATE,\n",
    "        total_amount NUMERIC(10,2),\n",
    "        status VARCHAR(50),\n",
    "        total_quantity INTEGER,\n",
    "        average_unit_price NUMERIC(10,2)\n",
    "    )\n",
    "    \"\"\")\n",
    "    pg_hook.insert_rows(temp_table, df.values.tolist(), df.columns.tolist(), commit_every=1000)\n",
    "    pg_hook.run(f\"\"\"\n",
    "    INSERT INTO {target_table}\n",
    "    SELECT * FROM {temp_table}\n",
    "    ON CONFLICT (order_id)\n",
    "    DO UPDATE SET\n",
    "        customer_id = EXCLUDED.customer_id,\n",
    "        order_date = EXCLUDED.order_date,\n",
    "        total_amount = EXCLUDED.total_amount,\n",
    "        status = EXCLUDED.status,\n",
    "        total_quantity = EXCLUDED.total_quantity,\n",
    "        average_unit_price = EXCLUDED.average_unit_price\n",
    "    \"\"\")\n",
    "    pg_hook.run(f\"DROP TABLE IF EXISTS {temp_table}\")\n",
    "    row_count = pg_hook.get_records(f\"SELECT COUNT(*) FROM {target_table}\")[0][0]\n",
    "    logging.info(f\"Chargement commandes terminé, {row_count} lignes dans {target_table}\")\n",
    "    return row_count\n",
    "\n",
    "# Vérification qualité des données\n",
    "def run_data_quality_checks(**context):\n",
    "    logging.info(\"Lancement des contrôles qualité\")\n",
    "    pg_hook = PostgresHook(postgres_conn_id='postgres_dwh')\n",
    "    checks = [\n",
    "        {\n",
    "            'name': 'Emails clients manquants',\n",
    "            'query': \"SELECT COUNT(*) FROM dim_customers WHERE email IS NULL OR email = ''\",\n",
    "            'threshold': 0,\n",
    "            'operator': 'lte'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Commandes sans clients',\n",
    "            'query': \"\"\"\n",
    "                SELECT COUNT(*) FROM fact_orders o\n",
    "                LEFT JOIN dim_customers c ON o.customer_id = c.customer_id\n",
    "                WHERE c.customer_id IS NULL\n",
    "            \"\"\",\n",
    "            'threshold': 0,\n",
    "            'operator': 'lte'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Montants négatifs',\n",
    "            'query': \"SELECT COUNT(*) FROM fact_orders WHERE total_amount < 0\",\n",
    "            'threshold': 0,\n",
    "            'operator': 'lte'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Nombre minimum de clients',\n",
    "            'query': \"SELECT COUNT(*) FROM dim_customers\",\n",
    "            'threshold': 100,\n",
    "            'operator': 'gte'\n",
    "        }\n",
    "    ]\n",
    "    failed_checks = []\n",
    "    for check in checks:\n",
    "        result = pg_hook.get_first(check['query'])[0]\n",
    "        if check['operator'] == 'lte' and result > check['threshold']:\n",
    "            failed_checks.append(f\"{check['name']} : {result} > {check['threshold']}\")\n",
    "        elif check['operator'] == 'gte' and result < check['threshold']:\n",
    "            failed_checks.append(f\"{check['name']} : {result} < {check['threshold']}\")\n",
    "        logging.info(f\"Résultat de {check['name']} : {result}\")\n",
    "\n",
    "    if failed_checks:\n",
    "        msg = \"Échec des contrôles qualité :\\n\" + \"\\n\".join(failed_checks)\n",
    "        logging.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    logging.info(\"Tous les contrôles qualité sont passés\")\n",
    "    return True\n",
    "\n",
    "# Notification en fin de process\n",
    "def send_success_notification(**context):\n",
    "    execution_date = context['execution_date']\n",
    "    dag_id = context['dag'].dag_id\n",
    "    message = f\"Le pipeline ETL {dag_id} a terminé avec succès pour la date {execution_date}\"\n",
    "    logging.info(f\"NOTIFICATION DE SUCCÈS : {message}\")\n",
    "    return message\n",
    "\n",
    "# Définition des tâches\n",
    "wait_for_data_sync = ExternalTaskSensor(\n",
    "    task_id='wait_for_data_sync',\n",
    "    external_dag_id='daily_data_sync',\n",
    "    external_task_id='complete_sync',\n",
    "    timeout=3600,\n",
    "    mode='reschedule',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "create_tables = PostgresOperator(\n",
    "    task_id='create_tables',\n",
    "    postgres_conn_id='postgres_dwh',\n",
    "    sql=\"\"\"CREATE TABLE IF NOT EXISTS dim_customers (...); CREATE TABLE IF NOT EXISTS fact_orders (...);\"\"\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "extract_customer_task = PythonOperator(\n",
    "    task_id='extract_customer_data',\n",
    "    python_callable=extract_customer_data,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "extract_order_task = PythonOperator(\n",
    "    task_id='extract_order_data',\n",
    "    python_callable=extract_order_data,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_data',\n",
    "    python_callable=transform_data,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load_customer_task = PythonOperator(\n",
    "    task_id='load_customer_data',\n",
    "    python_callable=load_customer_data,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load_order_task = PythonOperator(\n",
    "    task_id='load_order_data',\n",
    "    python_callable=load_order_data,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "data_quality_task = PythonOperator(\n",
    "    task_id='run_data_quality_checks',\n",
    "    python_callable=run_data_quality_checks,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "cleanup_task = BashOperator(\n",
    "    task_id='cleanup_temp_files',\n",
    "    bash_command='rm -f /tmp/customer_extract_*.csv /tmp/order_extract_*.csv /tmp/transformed_*.csv',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "notification_task = PythonOperator(\n",
    "    task_id='send_success_notification',\n",
    "    python_callable=send_success_notification,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Dépendances entre les tâches\n",
    "wait_for_data_sync >> create_tables\n",
    "create_tables >> [extract_customer_task, extract_order_task]\n",
    "[extract_customer_task, extract_order_task] >> transform_task\n",
    "transform_task >> [load_customer_task, load_order_task]\n",
    "[load_customer_task, load_order_task] >> data_quality_task\n",
    "data_quality_task >> cleanup_task >> notification_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
