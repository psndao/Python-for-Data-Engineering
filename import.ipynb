{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4356f9",
   "metadata": {},
   "source": [
    "<center><div style=\"width: 900px;  padding-top:5px; padding-bottom:10px;border: 3px solid #1625CB; text-align: left;background: #1625CB;\">\n",
    " <center> Pandas : la puissance de la manipulation des données </center>\n",
    "</div></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab09d9",
   "metadata": {},
   "source": [
    "Ces fonctions sont incroyablement polyvalentes et constituent la base de l'extraction de données dans les pipelines ETL Python. \n",
    "La fonction read_csv dispose à elle seule de plus de 50 paramètres pour gérer pratiquement tous les formats CSV. \n",
    "\n",
    "Parmi ses fonctionnalités clés, on trouve :\n",
    "\n",
    "-  Chunking : pour les fichiers volumineux, vous pouvez traiter les données par blocs grâce au paramètre chunksize\n",
    "-  Détection de la compression : gère automatiquement les fichiers gzippés, zippés ou bz2\n",
    "-  Analyse flexible : gère les données désordonnées avec des délimiteurs personnalisés, les valeurs manquantes, etc.\n",
    "-  Inférence de type : détecte automatiquement les types de données ou autorise la spécification manuelle\n",
    "\n",
    "\n",
    "La fonction read_sql se connecte directement aux bases de données, vous permettant d'exécuter des requêtes SQL et de récupérer les résultats sous forme de DataFrames. Cela crée une passerelle transparente entre le traitement des données SQL et Python.\n",
    "\n",
    "Pour les données JSON, omniprésentes dans les API modernes, read_json et json_normalize aident à analyser et à aplatir les structures imbriquées dans des formats tabulaires adaptés à l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #pandas (alias pd) : pour la manipulation de données tabulaires.\n",
    "import sqlite3  #sqlite3 : pour la gestion de bases de données SQLite.\n",
    "import requests #requests : pour effectuer des requêtes HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('data.csv', \n",
    "                    sep=',',\n",
    "                    header=0,\n",
    "                    usecols=['id', 'name', 'value'],\n",
    "                    dtype={'id': int, 'value': float},\n",
    "                    parse_dates=['date_column'],\n",
    "                    na_values=['NA', 'N/A', ''],\n",
    "                    nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('database.db') #sqlite3 : pour établir une connexion à une base de données SQLite.\n",
    "df_sql = pd.read_sql(\"SELECT * FROM table WHERE value > 100\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ca67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.read_json('data.json', orient='records') # Lit un fichier data.json structuré comme une liste de dictionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be86b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://api.example.com/data') # Effectue une requête GET à l'API\n",
    "df_api = pd.json_normalize(response.json()['results']) # Normalise les données JSON en un DataFrame plat"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
