{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef40e0f-e5b1-49db-a4ac-269ef3f6ff56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession créée avec succès\nVersion de Spark : 3.3.2\nURL Spark UI : http://10.172.249.176:40001\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Initialisation de la SparkSession – point d’entrée pour toutes les fonctionnalités Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark ETL Example\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Réduction de la verbosité des logs\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"SparkSession créée avec succès\")\n",
    "print(f\"Version de Spark : {spark.version}\")\n",
    "print(f\"URL Spark UI : {spark.sparkContext.uiWebUrl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544977b2-02fb-4fe5-83b0-bed7c6b3108a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Définition du schéma pour les ventes\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"store_id\", StringType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"price\", DoubleType(), False),\n",
    "    StructField(\"transaction_date\", TimestampType(), False),\n",
    "    StructField(\"payment_method\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feaae8c8-ee69-402c-b190-c78c0834538a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Génération de données aléatoires simulées\n",
    "num_records = 100000\n",
    "transaction_ids = [f\"T{i:08d}\" for i in range(1, num_records + 1)]\n",
    "customer_ids = [f\"C{random.randint(1, 5000):05d}\" for _ in range(num_records)]\n",
    "product_ids = [f\"P{random.randint(1, 500):05d}\" for _ in range(num_records)]\n",
    "store_ids = [f\"S{random.randint(1, 50):03d}\" for _ in range(num_records)]\n",
    "quantities = [random.randint(1, 10) for _ in range(num_records)]\n",
    "prices = [round(random.uniform(5.0, 500.0), 2) for _ in range(num_records)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a901913d-2974-42ff-8588-ebee36c5baed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dates de transaction sur les 30 derniers jours\n",
    "now = datetime.now()\n",
    "dates = [(now - timedelta(days=random.randint(0, 30),\n",
    "                          hours=random.randint(0, 23),\n",
    "                          minutes=random.randint(0, 59),\n",
    "                          seconds=random.randint(0, 59))) for _ in range(num_records)]\n",
    "\n",
    "payment_methods = [random.choice([\"Credit Card\", \"Debit Card\", \"Cash\", \"Mobile Payment\", None]) for _ in range(num_records)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ebe2c8-de95-4057-9e7e-83c8b4a1bd4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame des ventes créé avec 100000 lignes\nSchéma :\nroot\n |-- transaction_id: string (nullable = false)\n |-- customer_id: string (nullable = true)\n |-- product_id: string (nullable = false)\n |-- store_id: string (nullable = false)\n |-- quantity: integer (nullable = false)\n |-- price: double (nullable = false)\n |-- transaction_date: timestamp (nullable = false)\n |-- payment_method: string (nullable = true)\n\nExemple de données :\n+--------------+-----------+----------+--------+--------+------+-------------------------+--------------+\n|transaction_id|customer_id|product_id|store_id|quantity|price |transaction_date         |payment_method|\n+--------------+-----------+----------+--------+--------+------+-------------------------+--------------+\n|T00000001     |C03534     |P00125    |S025    |2       |366.51|2025-05-06 10:41:36.10273|Credit Card   |\n|T00000002     |C01994     |P00247    |S043    |1       |391.22|2025-05-06 08:17:44.10273|Mobile Payment|\n|T00000003     |C02254     |P00247    |S032    |3       |80.98 |2025-05-17 04:31:35.10273|Mobile Payment|\n|T00000004     |C02822     |P00339    |S025    |1       |10.15 |2025-05-15 10:18:46.10273|Mobile Payment|\n|T00000005     |C04182     |P00184    |S032    |1       |400.33|2025-05-11 00:46:47.10273|Cash          |\n+--------------+-----------+----------+--------+--------+------+-------------------------+--------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Création de la liste de lignes\n",
    "data = list(zip(transaction_ids, customer_ids, product_ids, store_ids, quantities, prices, dates, payment_methods))\n",
    "\n",
    "# Création de la DataFrame Spark\n",
    "sales_df = spark.createDataFrame(data, schema=sales_schema)\n",
    "\n",
    "print(f\"DataFrame des ventes créé avec {sales_df.count()} lignes\")\n",
    "print(\"Schéma :\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "print(\"Exemple de données :\")\n",
    "sales_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d46555-06b2-43d8-bab1-e359c7fcd19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- API DataFrame ---\nTop 5 des magasins par chiffre d'affaires :\n+--------+--------------+------------------+-----------------+\n|store_id|total_quantity|       total_sales|transaction_count|\n+--------+--------------+------------------+-----------------+\n|    S032|         11628|2962817.0700000003|             2115|\n|    S041|         11536|2956428.9699999993|             2088|\n|    S011|         11265| 2883221.579999999|             2057|\n|    S050|         11225|2880593.2000000007|             2028|\n|    S028|         11208|         2875564.4|             2030|\n+--------+--------------+------------------+-----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Enregistrement de la DataFrame comme table temporaire pour exécution SQL\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# 1. Analyse via l'API DataFrame\n",
    "print(\"\\n--- API DataFrame ---\")\n",
    "store_sales = sales_df.groupBy(\"store_id\") \\\n",
    "    .agg(\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        F.sum(F.col(\"quantity\") * F.col(\"price\")).alias(\"total_sales\"),\n",
    "        F.count(\"transaction_id\").alias(\"transaction_count\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"total_sales\"))\n",
    "\n",
    "print(\"Top 5 des magasins par chiffre d'affaires :\")\n",
    "store_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8efc94c-59f4-4c8a-95c6-a9fc0da181ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- SQL ---\nTop 5 des produits par chiffre d'affaires :\n+----------+--------------+------------------+-----------------+\n|product_id|total_quantity|       total_sales|transaction_count|\n+----------+--------------+------------------+-----------------+\n|    P00485|          1399| 367920.8800000001|              238|\n|    P00050|          1372|361906.63000000006|              238|\n|    P00464|          1353| 350878.6999999998|              231|\n|    P00299|          1259|344090.49000000005|              223|\n|    P00395|          1247| 340597.8699999999|              217|\n+----------+--------------+------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Analyse via SQL\n",
    "print(\"\\n--- SQL ---\")\n",
    "top_products_sql = \"\"\"\n",
    "SELECT \n",
    "    product_id,\n",
    "    SUM(quantity) as total_quantity,\n",
    "    SUM(quantity * price) as total_sales,\n",
    "    COUNT(DISTINCT transaction_id) as transaction_count\n",
    "FROM \n",
    "    sales\n",
    "GROUP BY \n",
    "    product_id\n",
    "ORDER BY \n",
    "    total_sales DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "top_products = spark.sql(top_products_sql)\n",
    "print(\"Top 5 des produits par chiffre d'affaires :\")\n",
    "top_products.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fdb6bfb-1666-4448-8cf5-3d7a9b6683a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nÉcriture en Parquet dans : sales_data_parquet\n"
     ]
    }
   ],
   "source": [
    "# 3. Lecture / Écriture de fichiers\n",
    "\n",
    "# Parquet\n",
    "output_dir = \"sales_data_parquet\"\n",
    "print(f\"\\nÉcriture en Parquet dans : {output_dir}\")\n",
    "sales_df.write.mode(\"overwrite\").parquet(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8d6ec9-4ff9-4d18-bca7-371aa4c78c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Configuration dynamique ---\nPartitions de shuffle actuelles : 4\nNouvelles partitions de shuffle : 8\n"
     ]
    }
   ],
   "source": [
    "# 4. Modification dynamique de configuration Spark\n",
    "print(\"\\n--- Configuration dynamique ---\")\n",
    "current_shuffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Partitions de shuffle actuelles : {current_shuffle_partitions}\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "new_shuffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Nouvelles partitions de shuffle : {new_shuffle_partitions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da19aca8-ce55-4a25-aa79-e05d78430f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Catalogue Spark ---\nTables temporaires :\n  sales_data (Temporaire : False)\n  sales (Temporaire : True)\n"
     ]
    }
   ],
   "source": [
    "# 5. Utilisation du catalogue Spark\n",
    "print(\"\\n--- Catalogue Spark ---\")\n",
    "print(\"Tables temporaires :\")\n",
    "for table in spark.catalog.listTables():\n",
    "    print(f\"  {table.name} (Temporaire : {table.isTemporary})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f2cfb8-7cf6-431f-aed1-fa6dec19bd75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nBases de données disponibles :\n  default\n  sales_db\n  ventes_db\n\nTables dans sales_db :\n  sales_history (Temporaire : False)\n  sales (Temporaire : True)\n\nRequête sur la table persistante :\n+---------+\n|row_count|\n+---------+\n|   100000|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Création d'une base et d'une table Hive persistante (si activé)\n",
    "try:\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS sales_db\")\n",
    "    spark.sql(\"USE sales_db\")\n",
    "    sales_df.write.mode(\"overwrite\").saveAsTable(\"sales_history\")\n",
    "\n",
    "    print(\"\\nBases de données disponibles :\")\n",
    "    for db in spark.catalog.listDatabases():\n",
    "        print(f\"  {db.name}\")\n",
    "\n",
    "    print(\"\\nTables dans sales_db :\")\n",
    "    for table in spark.catalog.listTables(\"sales_db\"):\n",
    "        print(f\"  {table.name} (Temporaire : {table.isTemporary})\")\n",
    "\n",
    "    print(\"\\nRequête sur la table persistante :\")\n",
    "    spark.sql(\"SELECT COUNT(*) as row_count FROM sales_db.sales_history\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Impossible de créer une table persistante : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61e0e70-c6b8-4ea0-a6d2-7db15ad4e071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Arrêt propre de Spark\n",
    "print(\"\\nArrêt de SparkSession\")\n",
    "spark.stop()\n",
    "print(\"SparkSession arrêtée\")\n",
    "\n",
    "# Nettoyage des fichiers temporaires\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "if os.path.exists(csv_dir):\n",
    "    shutil.rmtree(csv_dir)\n",
    "if os.path.exists(\"spark-warehouse\"):\n",
    "    shutil.rmtree(\"spark-warehouse\")\n",
    "print(\"Fichiers temporaires supprimés\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "etl_spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}